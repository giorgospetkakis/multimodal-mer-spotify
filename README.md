# CS2

Music Emotion Retrieval has been an active field of research for decades. Recently, Spotify released an online API that provides automatically generated emotion-related features about tracks in their music library. We used the MoodyLyrics4Q dataset to test the ability of these features to correctly classify emotions as defined by Russell's circumplex model of affect. We also tested classifiers trained only on external lyrics data. We hypothesized that supplementing the audio features collected from Spotify with textual information from the tracks' lyrics would improve our ability to recognize the emotion the tracks evoked. We proved our hypothesis, with our fusion models outperforming both unimodal approaches.
